2025-01-30 11:03:39,246 ----------------------------------------------------------------------------------------------------
2025-01-30 11:03:39,250 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28997, 1024)
        (position_embeddings): Embedding(512, 1024)
        (token_type_embeddings): Embedding(2, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-23): 24 x BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2025-01-30 11:03:39,250 ----------------------------------------------------------------------------------------------------
2025-01-30 11:03:39,250 Corpus: 5215 train + 1070 dev + 3658 test sentences
2025-01-30 11:03:39,250 ----------------------------------------------------------------------------------------------------
2025-01-30 11:03:39,251 Train:  5215 sentences
2025-01-30 11:03:39,251         (train_with_dev=False, train_with_test=False)
2025-01-30 11:03:39,251 ----------------------------------------------------------------------------------------------------
2025-01-30 11:03:39,251 Training Params:
2025-01-30 11:03:39,251  - learning_rate: "5e-06" 
2025-01-30 11:03:39,251  - mini_batch_size: "4"
2025-01-30 11:03:39,251  - max_epochs: "2"
2025-01-30 11:03:39,251  - shuffle: "True"
2025-01-30 11:03:39,251 ----------------------------------------------------------------------------------------------------
2025-01-30 11:03:39,251 Plugins:
2025-01-30 11:03:39,251  - LinearScheduler | warmup_fraction: '0.1'
2025-01-30 11:03:39,251 ----------------------------------------------------------------------------------------------------
2025-01-30 11:03:39,251 Final evaluation on model after last epoch (final-model.pt)
2025-01-30 11:03:39,251  - metric: "('micro avg', 'f1-score')"
2025-01-30 11:03:39,251 ----------------------------------------------------------------------------------------------------
2025-01-30 11:03:39,251 Computation:
2025-01-30 11:03:39,251  - compute on device: cpu
2025-01-30 11:03:39,252  - embedding storage: none
2025-01-30 11:03:39,252 ----------------------------------------------------------------------------------------------------
2025-01-30 11:03:39,252 Model training base path: "results/fine_tune_expert"
2025-01-30 11:03:39,252 ----------------------------------------------------------------------------------------------------
2025-01-30 11:03:39,252 ----------------------------------------------------------------------------------------------------
2025-01-30 11:36:33,956 epoch 1 - iter 130/1304 - loss 2.48020977 - time (sec): 1974.68 - samples/sec: 3.41 - lr: 0.000002 - momentum: 0.000000
2025-01-30 12:07:12,187 epoch 1 - iter 260/1304 - loss 1.73052414 - time (sec): 3812.92 - samples/sec: 3.53 - lr: 0.000005 - momentum: 0.000000
2025-01-30 12:37:33,086 epoch 1 - iter 390/1304 - loss 1.31669073 - time (sec): 5633.82 - samples/sec: 3.65 - lr: 0.000005 - momentum: 0.000000
2025-01-30 13:01:38,411 epoch 1 - iter 520/1304 - loss 1.10726592 - time (sec): 7079.15 - samples/sec: 3.80 - lr: 0.000004 - momentum: 0.000000
2025-01-30 13:55:02,337 epoch 1 - iter 650/1304 - loss 0.94587337 - time (sec): 10283.07 - samples/sec: 3.32 - lr: 0.000004 - momentum: 0.000000
